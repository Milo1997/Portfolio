# -*- coding: utf-8 -*-
"""Report Decision from data

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UdywPz2u_Z1c61DQtLXE8dI1rkzMmooZ
"""

if 'google.colab' in str(get_ipython()):

   !pip install geopandas
   !pip install pyarrow
   !pip install mapclassify
   !pip install rasterio
   !pip install pystac-client
   !pip install planetary-computer
   !pip install ipyleaflet
   !pip install imageio[pyav]

import pandas as pd
import geopandas as gpd
from shapely.geometry import Polygon, LineString, Point, MultiPoint
from google.colab import files
import zipfile
import io
import os
import requests
import rasterio

import plotly.express as px
from pystac_client import Client
from skimage import io

from pystac.extensions.eo import EOExtension as eo
import numpy as np
import os
import json
import geopandas as gpd
import pandas as pd
import numpy as np
import pystac_client
import planetary_computer as pc
import plotly.express as px
import plotly.io as pio
import rasterio
from rasterio import windows
from rasterio import features
from rasterio import warp
from skimage import io
import matplotlib.pyplot as plt
import seaborn as sns

!pip install rasterstats
from rasterstats import zonal_stats
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

"""#Sub-task 1: data import

The checks provide confidence because they allow the use to make visual inspection of the column names, columns values and the data types of each dataframe.
"""

if "central_asia_aoi" not in os.listdir(os.getcwd()):
    os.system('wget "https://github.com/data-analysis-3300-3003/data/raw/main/data/central_asia_aoi.zip"')
    os.system('unzip "central_asia_aoi.zip"')

os.system('unzip "central_asia_aoi.zip"')

geofiles_caoi = ['/content/central_asia_aoi/central_asia_aoi_1_crop_code.gpkg', '/content/central_asia_aoi/central_asia_aoi_2_crop_code.gpkg', '/content/central_asia_aoi/central_asia_aoi_3_crop_code.shp', '/content/central_asia_aoi/central_asia_aoi_4_crop_code.shp']


gdf1 = gpd.read_file(geofiles_caoi[0])
gdf2 = gpd.read_file(geofiles_caoi[1])
gdf3 = gpd.read_file(geofiles_caoi[2])
gdf4 = gpd.read_file(geofiles_caoi[3])


print(gdf1.head())
print(gdf2.head())
print(gdf3.head())
print(gdf4.head())


print(gdf1.columns)
print(gdf1.dtypes)
print(gdf2.columns)
print(gdf2.dtypes)
print(gdf3.columns)
print(gdf3.dtypes)
print(gdf4.columns)
print(gdf4.dtypes)


gdf1.plot()
gdf2.plot()
gdf3.plot()
gdf4.plot()

"""#Sub-task 2: data visualisation

This colour scale __*(Dark2)*__ was appropriate because it isn't a gradual colour scale and each crop variety has an easily and readily discernable assigned colour. This ultimately reduces the time it takes for the user to identify crop locations and their corresponding legend value.
"""

emap = gdf1.explore(
    column="crop_1",
    popup= True,
    cmap = "Dark2")
emap

gdf2.explore()
emap2 = gdf2.explore(
    column = 'crop_1',
    popup = True,
    cmap="Dark2",
    legend = True)
emap2

gdf3.explore()
emap3 = gdf3.explore(
    column="crop_1",
    popup=True,
    cmap="Dark2")
emap3

gdf4.explore()
emap4 = gdf4.explore(
    column="crop_1",
    popup = True,
    cmap="Dark2")
emap4

"""#Sub-task 3: data exploration"""

# Observation count gdf1
#print(gdf1.index)
print(f' in gdf1 the number of observations is: {gdf1.shape[0]}')

#Observaion count gdf2
#print(gdf2.index)
print(f' in gdf2 the number of observations is: {gdf2.shape[0]}')

#Observaion count gdf2
#print(gdf3.index)
print(f' in gdf3 the number of observations is: {gdf3.shape[0]}')

#Observaion count gdf2
#print(gdf4.index)
print(f' in gdf4 the number of observations is: {gdf4.shape[0]}')

gdf1.crs
gdf2.crs
gdf3.crs
gdf4.crs

print(f'the coordinate reference system of gdf1 is :{gdf1.crs}')
print(f'the coordinate reference system of gdf2 is :{gdf2.crs}')
print(f'the coordinate reference system of gdf3 is :{gdf3.crs}')
print(f'the coordinate reference system of gdf4 is :{gdf4.crs}')

"""#Sub-task 4: data transformation

*We* use unary_union because it is combining all of the polygon objects intoa single multipolygon, so when we use the envelope operation to calculate the boundary, it is calculating the boundary area of the entire multipolygon instead of each individual polygon object seperately.
"""

print(gdf1.geometry)

#performing a unary union and setting generating bounding box of dataframes using envelope operation
from shapely.geometry import box
boundbox1 = gpd.GeoSeries(gdf1.geometry.unary_union).envelope
boundbox2 = gpd.GeoSeries(gdf2.geometry.unary_union).envelope
boundbox3 = gpd.GeoSeries(gdf3.geometry.unary_union).envelope
boundbox4 = gpd.GeoSeries(gdf4.geometry.unary_union).envelope

# #setting the crs of the generated boundary boxes to maatch the original dataframes
boundbox1 = boundbox1.set_crs(gdf1.crs)
boundbox2 = boundbox2.set_crs(gdf2.crs)
boundbox3 = boundbox3.set_crs(gdf3.crs)
boundbox4 = boundbox4.set_crs(gdf4.crs)

boundbox1.plot()
boundbox2.plot()
boundbox3.plot()
boundbox4.plot()

"""#Sub-task 5: data transformation"""

boundbox1 = boundbox1.to_crs(epsg='4326')
boundbox2 = boundbox2.to_crs(epsg='4326')
boundbox3 = boundbox3.to_crs(epsg='4326')
boundbox4 = boundbox4.to_crs(epsg='4326')

print(f"The crs of boundbox1 is {boundbox1.crs}")
print(f"The crs of boundbox2 is {boundbox2.crs}")
print(f"The crs of boundbox3 is {boundbox3.crs}")
print(f"The crs of boundbox4 is {boundbox4.crs}")

# open a connection to the Microsoft Planetary Computer's root STAC catalog
pc_catalog = pystac_client.Client.open(
    url="https://planetarycomputer.microsoft.com/api/stac/v1"
    # modifier=planetary_computer.sign_inplace
)

m = gdf1.explore()
boundbox1.explore(m=m, color="red", style_kwds={"fillOpacity": 0})

m = gdf2.explore()
boundbox2.explore(m=m, color="red", style_kwds={"fillOpacity": 0})

m = gdf3.explore()
boundbox2.explore(m=m, color="red", style_kwds={"fillOpacity": 0})

m = gdf4.explore()
boundbox2.explore(m=m, color="red", style_kwds={"fillOpacity": 0})

#Converting polygon to N,S,W,E coordinates.
bbox1 = boundbox1.total_bounds.tolist()
bbox2 = boundbox2.total_bounds.tolist()
bbox3 = boundbox3.total_bounds.tolist()
bbox4 = boundbox4.total_bounds.tolist()

"""#Sub-task 6 - model: machine learning model strategy

Originally sentinel 2 data downloaded from microsoft planetary computer was preferred. This was due to the consideration of higher resolution satellite images (10 metres) compared with Landsat (30 metres), and the higher passover rate (every 5 days) for sentinel 2 as compared with Landsat (every 16 days). The logic behind this was that higher resolution and higher passover rates would lead to more accurate, less obsured datasets. However upon further evaluation __*Landsat-8 (from microsoft planetary computer) provided more complete datasets. Landsat-8 data from a period of 6 months (June to November) was used.*__ This decision was made to try and capture both multiple life stages of each crop (growth and maturation for cotton and wheat and maturation and senescence for alfalfa, vineyard and Orchard). This data will be used to create the input features __*'Normalized Difference Vegetation Index (NDVI)' and 'Enhanced Vegetation Index (EVI)*__. The thinking behind using NDVI was that this would capture the changes in plant health and plants density across the period in question. For EVI, the thinking was that it would be good at classifying crop types among other green varieties as it is sensitive in areas with dense vegetation (https://www.usgs.gov/landsat-missions/landsat-enhanced-vegetation-index#:~:text=Landsat%20Enhanced%20Vegetation%20Index%20(EVI,in%20areas%20with%20dense%20vegetation.) considering all fields evaluated were among other fields, this was an attractive option. Furthermore, atleast 2 of the crop types (cotton and Wheat) would not have a high EVI as cotton has a large white flower in the harvest stage and wheat turns yellow. With this in mind these predictors were furhter considered due to proved efficacy that a combination of these predictors is accurate in differentiating these exact and similar crop varieties at differrent points during their respective growing seasons (https://www.sciencedirect.com/science/article/pii/S0034425706004949).



Article with supporting findings: https://www.sciencedirect.com/science/article/pii/S1470160X20310633.

Further information: This report also created models using only NDVI and EVI seperately from the main combined model. This is to evaluate the efficacy of either as a standalone predictor and to further consider the positive or negative impact that each predictor has on the combined model.

Subtask 7 and Zonal Stats:
"""

months = range(5, 11)
year = 2018
year_next = 2018

# get the bounding box as a shapely object
clum_bbox_shapely = boundbox1[0]

# empty list to store ndarray of monthly ndvi values
ndvi_arr = []
evi_arr=[]

for i in months:
    print(f"processing month {i}")

    # here we convert numeric month indicators which can be single digit (e.g. Jan = 1)
    # to string month indicators which are two digits (e.g. Jan = "01")
    if i < 10:
        mnth = str(0) + str(i)
    else:
        mnth = str(i)

    # create month and year indicators for the start of the next month
    # we do this to constrain the search to one month at a time
    if i < 9:
        mnth_next = str(0) + str(i + 1)
    else:
        mnth_next = str(i + 1)

    if mnth_next == "13":
        mnth_next = "01"
        year_next = year + 1

    # create a time of interest search for each month in turn
    time_of_interest = f"{year}-{mnth}-01/{year_next}-{mnth_next}-01"

    # search the Microsoft Planetary computer to find Landsat assets matching our bounding box and time of interest
    search = pc_catalog.search(
        collections=["landsat-c2-l2"],
        intersects=clum_bbox_shapely, # use shapely object
        datetime=time_of_interest,
        query={
            "platform": {"in": ["landsat-8"]},
        }
    )

    # items is an item collection of Landsat assets that matched the search criteria
    items = search.item_collection()

    # get item with lowest cloud cover per-month

    # empty list - append the cloud cover each Landsat asset in the item collection to the list
    cloud_cover = []
    for cld in items:
        cloud_cover.append(eo.ext(cld).cloud_cover)

    # find the index location in the list of the asset with lowest cloud cover
    min_cloud_cover = min(cloud_cover)
    min_cloud_cover_idx = cloud_cover.index(min_cloud_cover)

    # get the Landsat asset with the lowest cloud cover
    least_cloudy = items[min_cloud_cover_idx]

    # get the signed URL for the red and nir least cloudy Landsat assets
    least_cloudy_red_href = pc.sign(least_cloudy.assets["red"].href)
    least_cloudy_nir08_href = pc.sign(least_cloudy.assets["nir08"].href)
    least_cloudy_blue_href = pc.sign(least_cloudy.assets["blue"].href)

    # for the first month get the shape of the array
    # for subequent months resample all arrays to this shape
    if i == 5:
        # read red band
        # open a connection to the COG using its signed link
        with rasterio.open(least_cloudy_red_href) as src:
            aoi_bounds = features.bounds(clum_bbox_shapely)
            warped_aoi_bounds = warp.transform_bounds("epsg:4326", src.crs, *aoi_bounds)
            aoi_window = windows.from_bounds(transform=src.transform, *warped_aoi_bounds)
            red = src.read(1, window=aoi_window)

            # get affine transform for the windowed read
            # this allows us to reproject the windowed read of data from COG file
            src_transform = src.transform
            win_transform = src.window_transform(aoi_window)

            # create a copy of the metadata for saving later
            out_meta = src.meta

            # get the shape of the red ndarray
            # force all other rasters to match this shape
            out_shape = red.shape

                        # create a copy of the metadata for saving later
            out_meta1 = src.meta

        # read nir band
        # open a connection to the COG using its signed link
        with rasterio.open(least_cloudy_nir08_href) as src:
            aoi_bounds = features.bounds(clum_bbox_shapely)
            warped_aoi_bounds = warp.transform_bounds("epsg:4326", src.crs, *aoi_bounds)
            aoi_window = windows.from_bounds(transform=src.transform, *warped_aoi_bounds)
            nir = src.read(1, out_shape=out_shape, window=aoi_window)

             # open a connection to the COG using its signed link
        with rasterio.open(least_cloudy_blue_href) as src:
            aoi_bounds = features.bounds(clum_bbox_shapely)
            warped_aoi_bounds = warp.transform_bounds("epsg:4326", src.crs, *aoi_bounds)
            aoi_window = windows.from_bounds(transform=src.transform, *warped_aoi_bounds)
            blu = src.read(1, out_shape=out_shape, window=aoi_window)


        # compute NDVI
        # cast to float as NDVI is bound between -1 and 1
        nir = nir.astype("float32")
        red = red.astype("float32")
        ndvi = (nir - red) / (nir + red)

        # compute envi EVI = 2.5 * ((NIR - Red) / (NIR + 6 * Red - 7.5 * Blue + 1))
        # cast to float as NDVI is bound between -1 and 1
        nir = nir.astype("float32")
        red = red.astype("float32")
        blu = blu.astype("float32")
        evi = 2.5 * ((nir - red) / (nir + 6 * red - 7.5 * blu + 1))

    else:
        # read red band
        # open a connection to the COG using its signed link
        with rasterio.open(least_cloudy_red_href) as src:
            aoi_bounds = features.bounds(clum_bbox_shapely)
            warped_aoi_bounds = warp.transform_bounds("epsg:4326", src.crs, *aoi_bounds)
            aoi_window = windows.from_bounds(transform=src.transform, *warped_aoi_bounds)
            # Here we pass out_shape into read to define the shape of the array data is read into
            red = src.read(1, out_shape=out_shape, window=aoi_window)

        # read nir band
        # open a connection to the COG using its signed link
        with rasterio.open(least_cloudy_nir08_href) as src:
            aoi_bounds = features.bounds(clum_bbox_shapely)
            warped_aoi_bounds = warp.transform_bounds("epsg:4326", src.crs, *aoi_bounds)
            aoi_window = windows.from_bounds(transform=src.transform, *warped_aoi_bounds)
            # Here we pass out_shape into read to define the shape of the array data is read into
            nir = src.read(1, out_shape=out_shape, window=aoi_window)

             # read nir band
        # open a connection to the COG using its signed link
        with rasterio.open(least_cloudy_blue_href) as src:
            aoi_bounds = features.bounds(clum_bbox_shapely)
            warped_aoi_bounds = warp.transform_bounds("epsg:4326", src.crs, *aoi_bounds)
            aoi_window = windows.from_bounds(transform=src.transform, *warped_aoi_bounds)
            # Here we pass out_shape into read to define the shape of the array data is read into
            blu = src.read(1, out_shape=out_shape, window=aoi_window)

        # compute NDVI
        # cast to float as NDVI is bound between -1 and 1
        nir = nir.astype("float32")
        red = red.astype("float32")
        ndvi = (nir - red) / (nir + red)

        # compute envi EVI = 2.5 * ((NIR - Red) / (NIR + 6 * Red - 7.5 * Blue + 1))
        # cast to float as NDVI is bound between -1 and 1
        nir = nir.astype("float32")
        red = red.astype("float32")
        blu = blu.astype("float32")
        evi = 2.5 * ((nir - red) / (nir + 6 * red - 7.5 * blu + 1))

    # append NDVI ndarray to list
    ndvi_arr.append(ndvi)
    evi_arr.append(evi)

# stack the ndvi ndarray's to create a multiband raster
ndvi_stacked_gdf1 = np.stack(ndvi_arr, axis=0)
evi_stacked_gdf1 = np.stack(evi_arr, axis=0)


# update meta for saving
out_meta["dtype"] = "float32"
out_meta["count"] = 6
out_meta["height"] = ndvi_arr[0].shape[0]
out_meta["width"] = ndvi_arr[0].shape[1]
out_meta["transform"] = win_transform
out_meta

# update meta for saving
out_meta1["dtype"] = "float32"
out_meta1["count"] = 6
out_meta1["height"] = evi_arr[0].shape[0]
out_meta1["width"] = evi_arr[0].shape[1]
out_meta1["transform"] = win_transform
out_meta


# zonal stats NDVI
zstats_meta = out_meta
zstats_meta["count"] = 1
# create a copy of clum_aoi to save zonal_stats results to
clum_aoi_ndvi_gdf1 = gdf1.copy()

month_idx = 1
for i in ndvi_arr:
    print(f"computing ndvi zonal stats for month {month_idx}")

    zstats = zonal_stats(
        gdf1.to_crs(zstats_meta["crs"]),
        i,
        affine=zstats_meta["transform"],
        stats=["mean"],
        all_touched=True
    )

    df_zstats = pd.DataFrame(zstats)
    clum_aoi_ndvi_gdf1[f"ndvi_{month_idx}"] = df_zstats.iloc[:, 0]
    month_idx += 1

clum_aoi_ndvi_gdf1.head()

# zonal stats EVI
zstats_meta = out_meta1
zstats_meta["count"] = 1
# create a copy of clum_aoi to save zonal_stats results to
clum_aoi_evi_gdf1 = gdf1.copy()

month_idx = 1
for i in evi_arr:
    print(f"computing evi zonal stats for month {month_idx}")

    zstats = zonal_stats(
        gdf1.to_crs(zstats_meta["crs"]),
        i,
        affine=zstats_meta["transform"],
        stats=["mean"],
        all_touched=True
    )

    df_zstats = pd.DataFrame(zstats)
    clum_aoi_evi_gdf1[f"evi_{month_idx}"] = df_zstats.iloc[:, 0]
    month_idx += 1

clum_aoi_evi_gdf1.head()




# visualise average monthly NDVI for row 4
# px.line(
#     x=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],
#     y=clum_aoi_ndvi_gdf1.loc[3, "ndvi_1":"ndvi_12"].tolist(),
#     labels = {
#         "y": "NDVI",
#         "x": "month"
#     }
# )

months = range(5, 11)
year = 2018
year_next = 2018

# get the bounding box as a shapely object
clum_bbox_shapely = boundbox2[0]

# empty list to store ndarray of monthly ndvi values
ndvi_arr = []
evi_arr = []

for i in months:
    print(f"processing month {i}")

    # here we convert numeric month indicators which can be single digit (e.g. Jan = 1)
    # to string month indicators which are two digits (e.g. Jan = "01")
    if i < 10:
        mnth = str(0) + str(i)
    else:
        mnth = str(i)

    # create month and year indicators for the start of the next month
    # we do this to constrain the search to one month at a time
    if i < 9:
        mnth_next = str(0) + str(i + 1)
    else:
        mnth_next = str(i + 1)

    if mnth_next == "13":
        mnth_next = "01"
        year_next = year + 1

    # create a time of interest search for each month in turn
    time_of_interest = f"{year}-{mnth}-01/{year_next}-{mnth_next}-01"

    # search the Microsoft Planetary computer to find Landsat assets matching our bounding box and time of interest
    search = pc_catalog.search(
        collections=["landsat-c2-l2"],
        intersects=clum_bbox_shapely, # use shapely object
        datetime=time_of_interest,
        query={
            "platform": {"in": ["landsat-8"]},
        }
    )

    # items is an item collection of Landsat assets that matched the search criteria
    items = search.item_collection()

    # get item with lowest cloud cover per-month

    # empty list - append the cloud cover each Landsat asset in the item collection to the list
    cloud_cover = []
    for cld in items:
        cloud_cover.append(eo.ext(cld).cloud_cover)

    # find the index location in the list of the asset with lowest cloud cover
    min_cloud_cover = min(cloud_cover)
    min_cloud_cover_idx = cloud_cover.index(min_cloud_cover)

    # get the Landsat asset with the lowest cloud cover
    least_cloudy = items[min_cloud_cover_idx]

    # get the signed URL for the red and nir least cloudy Landsat assets
    least_cloudy_red_href = pc.sign(least_cloudy.assets["red"].href)
    least_cloudy_nir08_href = pc.sign(least_cloudy.assets["nir08"].href)
    least_cloudy_blue_href = pc.sign(least_cloudy.assets["blue"].href)

    # for the first month get the shape of the array
    # for subequent months resample all arrays to this shape
    if i == 5:
        # read red band
        # open a connection to the COG using its signed link
        with rasterio.open(least_cloudy_red_href) as src:
            aoi_bounds = features.bounds(clum_bbox_shapely)
            warped_aoi_bounds = warp.transform_bounds("epsg:4326", src.crs, *aoi_bounds)
            aoi_window = windows.from_bounds(transform=src.transform, *warped_aoi_bounds)
            red = src.read(1, window=aoi_window)

            # get affine transform for the windowed read
            # this allows us to reproject the windowed read of data from COG file
            src_transform = src.transform
            win_transform = src.window_transform(aoi_window)

            # create a copy of the metadata for saving later
            out_meta = src.meta

            # get the shape of the red ndarray
            # force all other rasters to match this shape
            out_shape = red.shape

                        # create a copy of the metadata for saving later
            out_meta1 = src.meta

            # get the shape of the red ndarray
            # force all other rasters to match this shape
            out_shape = red.shape

        # read nir band
        # open a connection to the COG using its signed link
        with rasterio.open(least_cloudy_nir08_href) as src:
            aoi_bounds = features.bounds(clum_bbox_shapely)
            warped_aoi_bounds = warp.transform_bounds("epsg:4326", src.crs, *aoi_bounds)
            aoi_window = windows.from_bounds(transform=src.transform, *warped_aoi_bounds)
            nir = src.read(1, out_shape=out_shape, window=aoi_window)

             # open a connection to the COG using its signed link
        with rasterio.open(least_cloudy_blue_href) as src:
            aoi_bounds = features.bounds(clum_bbox_shapely)
            warped_aoi_bounds = warp.transform_bounds("epsg:4326", src.crs, *aoi_bounds)
            aoi_window = windows.from_bounds(transform=src.transform, *warped_aoi_bounds)
            blu = src.read(1, out_shape=out_shape, window=aoi_window)


        # compute NDVI
        # cast to float as NDVI is bound between -1 and 1
        nir = nir.astype("float32")
        red = red.astype("float32")
        ndvi = (nir - red) / (nir + red)

        # compute envi EVI = 2.5 * ((NIR - Red) / (NIR + 6 * Red - 7.5 * Blue + 1))
        # cast to float as NDVI is bound between -1 and 1
        nir = nir.astype("float32")
        red = red.astype("float32")
        blu = blu.astype("float32")
        evi = 2.5 * ((nir - red) / (nir + 6 * red - 7.5 * blu + 1))

    else:
        # read red band
        # open a connection to the COG using its signed link
        with rasterio.open(least_cloudy_red_href) as src:
            aoi_bounds = features.bounds(clum_bbox_shapely)
            warped_aoi_bounds = warp.transform_bounds("epsg:4326", src.crs, *aoi_bounds)
            aoi_window = windows.from_bounds(transform=src.transform, *warped_aoi_bounds)
            # Here we pass out_shape into read to define the shape of the array data is read into
            red = src.read(1, out_shape=out_shape, window=aoi_window)

        # read nir band
        # open a connection to the COG using its signed link
        with rasterio.open(least_cloudy_nir08_href) as src:
            aoi_bounds = features.bounds(clum_bbox_shapely)
            warped_aoi_bounds = warp.transform_bounds("epsg:4326", src.crs, *aoi_bounds)
            aoi_window = windows.from_bounds(transform=src.transform, *warped_aoi_bounds)
            # Here we pass out_shape into read to define the shape of the array data is read into
            nir = src.read(1, out_shape=out_shape, window=aoi_window)

             # read nir band
        # open a connection to the COG using its signed link
        with rasterio.open(least_cloudy_blue_href) as src:
            aoi_bounds = features.bounds(clum_bbox_shapely)
            warped_aoi_bounds = warp.transform_bounds("epsg:4326", src.crs, *aoi_bounds)
            aoi_window = windows.from_bounds(transform=src.transform, *warped_aoi_bounds)
            # Here we pass out_shape into read to define the shape of the array data is read into
            blu = src.read(1, out_shape=out_shape, window=aoi_window)

        # compute NDVI
        # cast to float as NDVI is bound between -1 and 1
        nir = nir.astype("float32")
        red = red.astype("float32")
        ndvi = (nir - red) / (nir + red)

        # compute envi EVI = 2.5 * ((NIR - Red) / (NIR + 6 * Red - 7.5 * Blue + 1))
        # cast to float as NDVI is bound between -1 and 1
        nir = nir.astype("float32")
        red = red.astype("float32")
        blu = blu.astype("float32")
        evi = 2.5 * ((nir - red) / (nir + 6 * red - 7.5 * blu + 1))

    # append NDVI ndarray to list
    ndvi_arr.append(ndvi)
    evi_arr.append(evi)

# stack the ndvi ndarray's to create a multiband raster
ndvi_stacked_gdf2 = np.stack(ndvi_arr, axis=0)
evi_stacked_gdf2 = np.stack(evi_arr, axis=0)


# update meta for saving
out_meta["dtype"] = "float32"
out_meta["count"] = 6
out_meta["height"] = ndvi_arr[0].shape[0]
out_meta["width"] = ndvi_arr[0].shape[1]
out_meta["transform"] = win_transform
out_meta

# update meta for saving
out_meta1["dtype"] = "float32"
out_meta1["count"] = 6
out_meta1["height"] = evi_arr[0].shape[0]
out_meta1["width"] = evi_arr[0].shape[1]
out_meta1["transform"] = win_transform
out_meta

# zonal stats NDVI
zstats_meta = out_meta1
zstats_meta["count"] = 1
# create a copy of clum_aoi to save zonal_stats results to
clum_aoi_ndvi_gdf2 = gdf2.copy()

month_idx = 1
for i in ndvi_arr:
    print(f"computing ndvi zonal stats for month {month_idx}")

    zstats = zonal_stats(
        gdf2.to_crs(zstats_meta["crs"]),
        i,
        affine=zstats_meta["transform"],
        stats=["mean"],
        all_touched=True
    )

    df_zstats = pd.DataFrame(zstats)
    clum_aoi_ndvi_gdf2[f"ndvi_{month_idx}"] = df_zstats.iloc[:, 0]
    month_idx += 1

clum_aoi_ndvi_gdf2.head()





# zonal stats EVI
zstats_meta = out_meta
zstats_meta["count"] = 1
# create a copy of clum_aoi to save zonal_stats results to
clum_aoi_evi_gdf2 = gdf2.copy()

month_idx = 1
for i in evi_arr:
    print(f"computing evi zonal stats for month {month_idx}")

    zstats = zonal_stats(
        gdf2.to_crs(zstats_meta["crs"]),
        i,
        affine=zstats_meta["transform"],
        stats=["mean"],
        all_touched=True
    )

    df_zstats = pd.DataFrame(zstats)
    clum_aoi_evi_gdf2[f"evi_{month_idx}"] = df_zstats.iloc[:, 0]
    month_idx += 1

clum_aoi_evi_gdf2.head()


# # visualise average monthly NDVI for row 4
# px.line(
#     x=[5, 6, 7, 8, 9, 10, 11, 12],
#     y=clum_aoi_ndvi_gdf2.loc[3, "ndvi_1":"ndvi_8"].tolist(),
#     labels = {
#         "y": "NDVI",
#         "x": "month"
#     }
# )

months = range(5, 11)
year = 2018
year_next = 2018

# get the bounding box as a shapely object
clum_bbox_shapely = boundbox3[0]

# empty list to store ndarray of monthly ndvi values
ndvi_arr = []
evi_arr = []

for i in months:
    print(f"processing month {i}")

    # here we convert numeric month indicators which can be single digit (e.g. Jan = 1)
    # to string month indicators which are two digits (e.g. Jan = "01")
    if i < 10:
        mnth = str(0) + str(i)
    else:
        mnth = str(i)

    # create month and year indicators for the start of the next month
    # we do this to constrain the search to one month at a time
    if i < 9:
        mnth_next = str(0) + str(i + 1)
    else:
        mnth_next = str(i + 1)

    if mnth_next == "13":
        mnth_next = "01"
        year_next = year + 1

    # create a time of interest search for each month in turn
    time_of_interest = f"{year}-{mnth}-01/{year_next}-{mnth_next}-01"

    # search the Microsoft Planetary computer to find Landsat assets matching our bounding box and time of interest
    search = pc_catalog.search(
        collections=["landsat-c2-l2"],
        intersects=clum_bbox_shapely, # use shapely object
        datetime=time_of_interest,
        query={
            "platform": {"in": ["landsat-8"]},
        }
    )

    # items is an item collection of Landsat assets that matched the search criteria
    items = search.item_collection()

    # get item with lowest cloud cover per-month

    # empty list - append the cloud cover each Landsat asset in the item collection to the list
    cloud_cover = []
    for cld in items:
        cloud_cover.append(eo.ext(cld).cloud_cover)

    # find the index location in the list of the asset with lowest cloud cover
    min_cloud_cover = min(cloud_cover)
    min_cloud_cover_idx = cloud_cover.index(min_cloud_cover)

    # get the Landsat asset with the lowest cloud cover
    least_cloudy = items[min_cloud_cover_idx]

    # get the signed URL for the red and nir least cloudy Landsat assets
    least_cloudy_red_href = pc.sign(least_cloudy.assets["red"].href)
    least_cloudy_nir08_href = pc.sign(least_cloudy.assets["nir08"].href)
    least_cloudy_blue_href = pc.sign(least_cloudy.assets["blue"].href)

    # for the first month get the shape of the array
    # for subequent months resample all arrays to this shape
    if i == 5:
        # read red band
        # open a connection to the COG using its signed link
        with rasterio.open(least_cloudy_red_href) as src:
            aoi_bounds = features.bounds(clum_bbox_shapely)
            warped_aoi_bounds = warp.transform_bounds("epsg:4326", src.crs, *aoi_bounds)
            aoi_window = windows.from_bounds(transform=src.transform, *warped_aoi_bounds)
            red = src.read(1, window=aoi_window)

            # get affine transform for the windowed read
            # this allows us to reproject the windowed read of data from COG file
            src_transform = src.transform
            win_transform = src.window_transform(aoi_window)

            # create a copy of the metadata for saving later
            out_meta = src.meta

            # get the shape of the red ndarray
            # force all other rasters to match this shape
            out_shape = red.shape

                        # create a copy of the metadata for saving later
            out_meta1 = src.meta

            # get the shape of the red ndarray
            # force all other rasters to match this shape
            out_shape = red.shape

        # read nir band
        # open a connection to the COG using its signed link
        with rasterio.open(least_cloudy_nir08_href) as src:
            aoi_bounds = features.bounds(clum_bbox_shapely)
            warped_aoi_bounds = warp.transform_bounds("epsg:4326", src.crs, *aoi_bounds)
            aoi_window = windows.from_bounds(transform=src.transform, *warped_aoi_bounds)
            nir = src.read(1, out_shape=out_shape, window=aoi_window)

             # open a connection to the COG using its signed link
        with rasterio.open(least_cloudy_blue_href) as src:
            aoi_bounds = features.bounds(clum_bbox_shapely)
            warped_aoi_bounds = warp.transform_bounds("epsg:4326", src.crs, *aoi_bounds)
            aoi_window = windows.from_bounds(transform=src.transform, *warped_aoi_bounds)
            blu = src.read(1, out_shape=out_shape, window=aoi_window)


        # compute NDVI
        # cast to float as NDVI is bound between -1 and 1
        nir = nir.astype("float32")
        red = red.astype("float32")
        ndvi = (nir - red) / (nir + red)

        # compute envi EVI = 2.5 * ((NIR - Red) / (NIR + 6 * Red - 7.5 * Blue + 1))
        # cast to float as NDVI is bound between -1 and 1
        nir = nir.astype("float32")
        red = red.astype("float32")
        blu = blu.astype("float32")
        evi = 2.5 * ((nir - red) / (nir + 6 * red - 7.5 * blu + 1))

    else:
        # read red band
        # open a connection to the COG using its signed link
        with rasterio.open(least_cloudy_red_href) as src:
            aoi_bounds = features.bounds(clum_bbox_shapely)
            warped_aoi_bounds = warp.transform_bounds("epsg:4326", src.crs, *aoi_bounds)
            aoi_window = windows.from_bounds(transform=src.transform, *warped_aoi_bounds)
            # Here we pass out_shape into read to define the shape of the array data is read into
            red = src.read(1, out_shape=out_shape, window=aoi_window)

        # read nir band
        # open a connection to the COG using its signed link
        with rasterio.open(least_cloudy_nir08_href) as src:
            aoi_bounds = features.bounds(clum_bbox_shapely)
            warped_aoi_bounds = warp.transform_bounds("epsg:4326", src.crs, *aoi_bounds)
            aoi_window = windows.from_bounds(transform=src.transform, *warped_aoi_bounds)
            # Here we pass out_shape into read to define the shape of the array data is read into
            nir = src.read(1, out_shape=out_shape, window=aoi_window)

             # read nir band
        # open a connection to the COG using its signed link
        with rasterio.open(least_cloudy_blue_href) as src:
            aoi_bounds = features.bounds(clum_bbox_shapely)
            warped_aoi_bounds = warp.transform_bounds("epsg:4326", src.crs, *aoi_bounds)
            aoi_window = windows.from_bounds(transform=src.transform, *warped_aoi_bounds)
            # Here we pass out_shape into read to define the shape of the array data is read into
            blu = src.read(1, out_shape=out_shape, window=aoi_window)

        # compute NDVI
        # cast to float as NDVI is bound between -1 and 1
        nir = nir.astype("float32")
        red = red.astype("float32")
        ndvi = (nir - red) / (nir + red)

        # compute envi EVI = 2.5 * ((NIR - Red) / (NIR + 6 * Red - 7.5 * Blue + 1))
        # cast to float as NDVI is bound between -1 and 1
        nir = nir.astype("float32")
        red = red.astype("float32")
        blu = blu.astype("float32")
        evi = 2.5 * ((nir - red) / (nir + 6 * red - 7.5 * blu + 1))

    # append NDVI ndarray to list
    ndvi_arr.append(ndvi)
    evi_arr.append(evi)

# stack the ndvi ndarray's to create a multiband raster
ndvi_stacked_gdf3 = np.stack(ndvi_arr, axis=0)
evi_stacked_gdf3 = np.stack(evi_arr, axis=0)


# update meta for saving
out_meta["dtype"] = "float32"
out_meta["count"] = 6
out_meta["height"] = ndvi_arr[0].shape[0]
out_meta["width"] = ndvi_arr[0].shape[1]
out_meta["transform"] = win_transform
out_meta

# update meta for saving
out_meta1["dtype"] = "float32"
out_meta1["count"] = 6
out_meta1["height"] = evi_arr[0].shape[0]
out_meta1["width"] = evi_arr[0].shape[1]
out_meta1["transform"] = win_transform
out_meta



# zonal stats NDVI
zstats_meta = out_meta
zstats_meta["count"] = 1
# create a copy of clum_aoi to save zonal_stats results to
clum_aoi_ndvi_gdf3 = gdf3.copy()

month_idx = 1
for i in ndvi_arr:
    print(f"computing ndvi zonal stats for month {month_idx}")

    zstats = zonal_stats(
        gdf3.to_crs(zstats_meta["crs"]),
        i,
        affine=zstats_meta["transform"],
        stats=["mean"],
        all_touched=True
    )

    df_zstats = pd.DataFrame(zstats)
    clum_aoi_ndvi_gdf3[f"ndvi_{month_idx}"] = df_zstats.iloc[:, 0]
    month_idx += 1

clum_aoi_ndvi_gdf3.head()

# zonal stats EVI
zstats_meta = out_meta1
zstats_meta["count"] = 1
# create a copy of clum_aoi to save zonal_stats results to
clum_aoi_evi_gdf3 = gdf3.copy()

month_idx = 1
for i in evi_arr:
    print(f"computing evi zonal stats for month {month_idx}")

    zstats = zonal_stats(
        gdf3.to_crs(zstats_meta["crs"]),
        i,
        affine=zstats_meta["transform"],
        stats=["mean"],
        all_touched=True
    )

    df_zstats = pd.DataFrame(zstats)
    clum_aoi_evi_gdf3[f"evi_{month_idx}"] = df_zstats.iloc[:, 0]
    month_idx += 1

clum_aoi_evi_gdf3.head()


# visualise average monthly NDVI for row 4
# px.line(
#     x=[5, 6, 7, 8, 9, 10, 11, 12],
#     y=clum_aoi_ndvi_gdf3.loc[3, "ndvi_1":"ndvi_8"].tolist(),
#     labels = {
#         "y": "NDVI",
#         "x": "month"
#     }
# )

months = range(5, 11)
year = 2018
year_next = 2018

# get the bounding box as a shapely object
clum_bbox_shapely = boundbox4[0]

# empty list to store ndarray of monthly ndvi values
ndvi_arr = []
evi_arr = []

for i in months:
    print(f"processing month {i}")

    # here we convert numeric month indicators which can be single digit (e.g. Jan = 1)
    # to string month indicators which are two digits (e.g. Jan = "01")
    if i < 10:
        mnth = str(0) + str(i)
    else:
        mnth = str(i)

    # create month and year indicators for the start of the next month
    # we do this to constrain the search to one month at a time
    if i < 9:
        mnth_next = str(0) + str(i + 1)
    else:
        mnth_next = str(i + 1)

    if mnth_next == "13":
        mnth_next = "01"
        year_next = year + 1

    # create a time of interest search for each month in turn
    time_of_interest = f"{year}-{mnth}-01/{year_next}-{mnth_next}-01"

    # search the Microsoft Planetary computer to find Landsat assets matching our bounding box and time of interest
    search = pc_catalog.search(
        collections=["landsat-c2-l2"],
        intersects=clum_bbox_shapely, # use shapely object
        datetime=time_of_interest,
        query={
            "platform": {"in": ["landsat-8"]},
        }
    )

    # items is an item collection of Landsat assets that matched the search criteria
    items = search.item_collection()

    # get item with lowest cloud cover per-month

    # empty list - append the cloud cover each Landsat asset in the item collection to the list
    cloud_cover = []
    for cld in items:
        cloud_cover.append(eo.ext(cld).cloud_cover)

    # find the index location in the list of the asset with lowest cloud cover
    min_cloud_cover = min(cloud_cover)
    min_cloud_cover_idx = cloud_cover.index(min_cloud_cover)

    # get the Landsat asset with the lowest cloud cover
    least_cloudy = items[min_cloud_cover_idx]

    # get the signed URL for the red and nir least cloudy Landsat assets
    least_cloudy_red_href = pc.sign(least_cloudy.assets["red"].href)
    least_cloudy_nir08_href = pc.sign(least_cloudy.assets["nir08"].href)
    least_cloudy_blue_href = pc.sign(least_cloudy.assets["blue"].href)


    # for the first month get the shape of the array
    # for subequent months resample all arrays to this shape
    if i == 5:
        # read red band
        # open a connection to the COG using its signed link
        with rasterio.open(least_cloudy_red_href) as src:
            aoi_bounds = features.bounds(clum_bbox_shapely)
            warped_aoi_bounds = warp.transform_bounds("epsg:4326", src.crs, *aoi_bounds)
            aoi_window = windows.from_bounds(transform=src.transform, *warped_aoi_bounds)
            red = src.read(1, window=aoi_window)

            # get affine transform for the windowed read
            # this allows us to reproject the windowed read of data from COG file
            src_transform = src.transform
            win_transform = src.window_transform(aoi_window)

            # create a copy of the metadata for saving later
            out_meta = src.meta

            # get the shape of the red ndarray
            # force all other rasters to match this shape
            out_shape = red.shape

                        # create a copy of the metadata for saving later
            out_meta1 = src.meta

            # get the shape of the red ndarray
            # force all other rasters to match this shape
            out_shape = red.shape

        # read nir band
        # open a connection to the COG using its signed link
        with rasterio.open(least_cloudy_nir08_href) as src:
            aoi_bounds = features.bounds(clum_bbox_shapely)
            warped_aoi_bounds = warp.transform_bounds("epsg:4326", src.crs, *aoi_bounds)
            aoi_window = windows.from_bounds(transform=src.transform, *warped_aoi_bounds)
            nir = src.read(1, out_shape=out_shape, window=aoi_window)

             # open a connection to the COG using its signed link
        with rasterio.open(least_cloudy_blue_href) as src:
            aoi_bounds = features.bounds(clum_bbox_shapely)
            warped_aoi_bounds = warp.transform_bounds("epsg:4326", src.crs, *aoi_bounds)
            aoi_window = windows.from_bounds(transform=src.transform, *warped_aoi_bounds)
            blu = src.read(1, out_shape=out_shape, window=aoi_window)


        # compute NDVI
        # cast to float as NDVI is bound between -1 and 1
        nir = nir.astype("float32")
        red = red.astype("float32")
        ndvi = (nir - red) / (nir + red)

        # compute envi EVI = 2.5 * ((NIR - Red) / (NIR + 6 * Red - 7.5 * Blue + 1))
        # cast to float as NDVI is bound between -1 and 1
        nir = nir.astype("float32")
        red = red.astype("float32")
        blu = blu.astype("float32")
        evi = 2.5 * ((nir - red) / (nir + 6 * red - 7.5 * blu + 1))

    else:
        # read red band
        # open a connection to the COG using its signed link
        with rasterio.open(least_cloudy_red_href) as src:
            aoi_bounds = features.bounds(clum_bbox_shapely)
            warped_aoi_bounds = warp.transform_bounds("epsg:4326", src.crs, *aoi_bounds)
            aoi_window = windows.from_bounds(transform=src.transform, *warped_aoi_bounds)
            # Here we pass out_shape into read to define the shape of the array data is read into
            red = src.read(1, out_shape=out_shape, window=aoi_window)

        # read nir band
        # open a connection to the COG using its signed link
        with rasterio.open(least_cloudy_nir08_href) as src:
            aoi_bounds = features.bounds(clum_bbox_shapely)
            warped_aoi_bounds = warp.transform_bounds("epsg:4326", src.crs, *aoi_bounds)
            aoi_window = windows.from_bounds(transform=src.transform, *warped_aoi_bounds)
            # Here we pass out_shape into read to define the shape of the array data is read into
            nir = src.read(1, out_shape=out_shape, window=aoi_window)

             # read nir band
        # open a connection to the COG using its signed link
        with rasterio.open(least_cloudy_blue_href) as src:
            aoi_bounds = features.bounds(clum_bbox_shapely)
            warped_aoi_bounds = warp.transform_bounds("epsg:4326", src.crs, *aoi_bounds)
            aoi_window = windows.from_bounds(transform=src.transform, *warped_aoi_bounds)
            # Here we pass out_shape into read to define the shape of the array data is read into
            blu = src.read(1, out_shape=out_shape, window=aoi_window)

        # compute NDVI
        # cast to float as NDVI is bound between -1 and 1
        nir = nir.astype("float32")
        red = red.astype("float32")
        ndvi = (nir - red) / (nir + red)

        # compute envi EVI = 2.5 * ((NIR - Red) / (NIR + 6 * Red - 7.5 * Blue + 1))
        # cast to float as NDVI is bound between -1 and 1
        nir = nir.astype("float32")
        red = red.astype("float32")
        blu = blu.astype("float32")
        evi = 2.5 * ((nir - red) / (nir + 6 * red - 7.5 * blu + 1))

    # append NDVI ndarray to list
    ndvi_arr.append(ndvi)
    evi_arr.append(evi)

# stack the ndvi ndarray's to create a multiband raster
ndvi_stacked_gdf4 = np.stack(ndvi_arr, axis=0)
evi_stacked_gdf4 = np.stack(evi_arr, axis=0)
print(f"the shape of ndvi_arr is {ndvi_stacked_gdf4.shape}")


# update meta for saving
out_meta["dtype"] = "float32"
out_meta["count"] = 6
out_meta["height"] = ndvi_arr[0].shape[0]
out_meta["width"] = ndvi_arr[0].shape[1]
out_meta["transform"] = win_transform
out_meta

# update meta for saving
out_meta1["dtype"] = "float32"
out_meta1["count"] = 6
out_meta1["height"] = evi_arr[0].shape[0]
out_meta1["width"] = evi_arr[0].shape[1]
out_meta1["transform"] = win_transform
out_meta

# zonal stats NDVI
zstats_meta = out_meta
zstats_meta["count"] = 1
# create a copy of clum_aoi to save zonal_stats results to
clum_aoi_ndvi_gdf4 = gdf4.copy()

month_idx = 1
for i in ndvi_arr:
    print(f"computing ndvi zonal stats for month {month_idx}")

    zstats = zonal_stats(
        gdf4.to_crs(zstats_meta["crs"]),
        i,
        affine=zstats_meta["transform"],
        stats=["mean"],
        all_touched=True
    )

    df_zstats = pd.DataFrame(zstats)
    clum_aoi_ndvi_gdf4[f"ndvi_{month_idx}"] = df_zstats.iloc[:, 0]
    month_idx += 1

clum_aoi_ndvi_gdf4.head()


# zonal stats EVI
zstats_meta = out_meta1
zstats_meta["count"] = 1
# create a copy of clum_aoi to save zonal_stats results to
clum_aoi_evi_gdf4 = gdf4.copy()

month_idx = 1
for i in evi_arr:
    print(f"computing evi zonal stats for month {month_idx}")

    zstats = zonal_stats(
        gdf4.to_crs(zstats_meta["crs"]),
        i,
        affine=zstats_meta["transform"],
        stats=["mean"],
        all_touched=True
    )

    df_zstats = pd.DataFrame(zstats)
    clum_aoi_evi_gdf4[f"evi_{month_idx}"] = df_zstats.iloc[:, 0]
    month_idx += 1

clum_aoi_evi_gdf4.head()

# # visualise average monthly NDVI for row 4
# px.line(
#     x=[5, 6, 7, 8, 9, 10, 11, 12],
#     y=clum_aoi_ndvi_gdf4.loc[3, "ndvi_1":"ndvi_8"].tolist(),
#     labels = {
#         "y": "NDVI",
#         "x": "month"
#     }
# )

out_meta

"""#Subtask 8 (Zonal Stats code is under Subtask 7)
In subtask 8 boxplots were used to visualise the extent of the occurence of outliers. Animated visualisation of the NDVI and EVI layers were generated to view the integrity of data in the layers formed in subtask 7. Interquartile range method was use where the 0.25th and 0.75th percentile of the data were calulated, using this the interquartile range was calculated (subtracting and adding 1.5 to interquartile raange) to establish lower and upper bounds. .mask() was used to replace the values less than and greater than the lower and upper bounds with 'NaN'. .dropna() was then used to remove 'NaN' values. This was defined in a group function. The gdf was grouped by EVI and NDVI values and then the group function was applied to each predictor group.  
"""

# let's animate change NDVI over the growing season
fig = px.imshow(
    ndvi_stacked_gdf1,
    animation_frame=0, # here we are specifying axis=2 which is the bands (weeks) dimension.
    color_continuous_scale="viridis",
    range_color=[0, 0.5],
    height=600,
    width=800)
fig.update_xaxes(showticklabels=False)
fig.update_yaxes(showticklabels=False)
fig.show()

# let's animate change NDVI over the growing season
fig = px.imshow(
    ndvi_stacked_gdf2,
    animation_frame=0, # here we are specifying axis=2 which is the bands (weeks) dimension.
    color_continuous_scale="viridis",
    range_color=[0, 0.5],
    height=600,
    width=800)
fig.update_xaxes(showticklabels=False)
fig.update_yaxes(showticklabels=False)
fig.show()

# animate change NDVI over the growing season
fig = px.imshow(
    ndvi_stacked_gdf3,
    animation_frame=0, # here we are specifying axis=2 which is the bands (weeks) dimension.
    color_continuous_scale="viridis",
    range_color=[0, 0.5],
    height=600,
    width=800)
fig.update_xaxes(showticklabels=False)
fig.update_yaxes(showticklabels=False)
fig.show()

# animate change NDVI over the growing season
fig = px.imshow(
    ndvi_stacked_gdf4,
    animation_frame=0, # here we are specifying axis=2 which is the bands (weeks) dimension.
    color_continuous_scale="viridis",
    range_color=[0, 0.5],
    height=600,
    width=800)
fig.update_xaxes(showticklabels=False)
fig.update_yaxes(showticklabels=False)
fig.show()

#animate change EVI over the growing season
fig = px.imshow(
    evi_stacked_gdf1,
    animation_frame=0, # here we are specifying axis=2 which is the bands (weeks) dimension.
    color_continuous_scale="greens",
    range_color=[0, 0.5],
    height=600,
    width=800)
fig.update_xaxes(showticklabels=False)
fig.update_yaxes(showticklabels=False)
fig.show()

# let's animate change GNDVI over the growing season
fig = px.imshow(
    evi_stacked_gdf2,
    animation_frame=0, # here we are specifying axis=2 which is the bands (weeks) dimension.
    color_continuous_scale="greens",
    range_color=[0, 0.5],
    height=600,
    width=800)
fig.update_xaxes(showticklabels=False)
fig.update_yaxes(showticklabels=False)
fig.show()

# let's animate change GNDVI over the growing season
fig = px.imshow(
    evi_stacked_gdf3,
    animation_frame=0, # here we are specifying axis=2 which is the bands (weeks) dimension.
    color_continuous_scale="greens",
    range_color=[0, 0.5],
    height=600,
    width=800)
fig.update_xaxes(showticklabels=False)
fig.update_yaxes(showticklabels=False)
fig.show()

# let's animate change GNDVI over the growing season
fig = px.imshow(
    evi_stacked_gdf4,
    animation_frame=0, # here we are specifying axis=2 which is the bands (weeks) dimension.
    color_continuous_scale="greens",
    range_color=[0, 0.5],
    height=600,
    width=800)
fig.update_xaxes(showticklabels=False)
fig.update_yaxes(showticklabels=False)
fig.show()



#concatenating all of the NDVI gdfs for each correlating gdf
geoframe1 = clum_aoi_ndvi_gdf1.reset_index(drop=True)
geoframe2 = clum_aoi_ndvi_gdf2.reset_index(drop=True)
geoframe3 = clum_aoi_ndvi_gdf3.reset_index(drop=True)
geoframe4 = clum_aoi_ndvi_gdf4.reset_index(drop=True)

#concatenating all of the EVI gdfs for each correlating gdf
geoframe_evi_1 = clum_aoi_evi_gdf1.reset_index(drop=True)
geoframe_evi_2 = clum_aoi_evi_gdf2.reset_index(drop=True)
geoframe_evi_3 = clum_aoi_evi_gdf3.reset_index(drop=True)
geoframe_evi_4 = clum_aoi_evi_gdf4.reset_index(drop=True)
# joined_df = pd.concat([df1, df2.set_axis(df1.index)], axis=1)
gdf_concat_ndvi = pd.concat([geoframe1, geoframe2, geoframe3, geoframe4])
gdf_concat_evi1 = pd.concat([geoframe_evi_1, geoframe_evi_2, geoframe_evi_3, geoframe_evi_4])
gdf_concat_evi = gdf_concat_evi1.drop(['geometry', 'crop1_code', 'region', 'crop_1', 'country','year', 'season'],axis =1)

gdf_concat = pd.concat([gdf_concat_ndvi, gdf_concat_evi.set_axis(gdf_concat_ndvi.index)], axis=1)
gdf_concat

gdf_concat.describe()

y_columns = ['ndvi_1', 'ndvi_2', 'ndvi_3', 'ndvi_4', 'ndvi_5', 'ndvi_6', 'evi_1','evi_2','evi_3','evi_4','evi_5','evi_6',]
evi_y = ['evi_1','evi_2','evi_3','evi_4','evi_5','evi_6']
ndvi_y = ['ndvi_1', 'ndvi_2', 'ndvi_3', 'ndvi_4', 'ndvi_5', 'ndvi_6']
reference_column = ['crop_1']

# Extract the reference and data columns from the GeoDataFrame EVI ONLY
reference_values = gdf_concat[reference_column].squeeze()
data_values = gdf_concat[evi_y]

# Create separate box plots for each crop category
plt.figure(figsize=(10, 6))

for crop_category in reference_values.unique():
    crop_data = data_values[reference_values == crop_category]
    crop_data.boxplot(rot=45, patch_artist=True, boxprops=dict(facecolor='C0'))

    plt.xticks(range(1, len(evi_y) + 1), evi_y)
    plt.xlabel('EVI Columns')
    plt.ylabel('EVI Values')
    plt.title(f'Boxplot of NDVI Values for {crop_category}')
    plt.show()

# Extract the reference and data columns from the GeoDataFrame EVI ONLY
reference_values = gdf_concat[reference_column].squeeze()
data_values = gdf_concat[ndvi_y]

# Create separate box plots for each crop category
plt.figure(figsize=(10, 6))

for crop_category in reference_values.unique():
    crop_data = data_values[reference_values == crop_category]
    crop_data.boxplot(rot=45, patch_artist=True, boxprops=dict(facecolor='C0'))

    plt.xticks(range(1, len(ndvi_y) + 1), ndvi_y)
    plt.xlabel('EVI Columns')
    plt.ylabel('EVI Values')
    plt.title(f'Boxplot of NDVI Values for {crop_category}')
    plt.show()

"""Cleaning gdf NDVI and EVI seperately"""

# Cleaning outlying values for JUST FOR EVI
reference_column = 'crop_1'
data_columns = ['evi_1', 'evi_2', 'evi_3', 'evi_4', 'evi_5', 'evi_6']

# Remove minimally occurring indexes
labels_to_remove = ['onions', 'beans', 'rice', 'fallow', 'maize', 'vegetables']
#NOTE: '~' negates boolean, if there wasnt a '~' it would keep values correlating to 'lables_to_remove'
gdf_concat_evi = gdf_concat_evi1[~gdf_concat_evi1['crop_1'].isin(labels_to_remove)]

# Calculate the lower and upper bounds for outlier removal (e.g., using 1.5 times the interquartile range)
def remove_outliers(group):
    q1 = group.quantile(0.25)
    q3 = group.quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    return group.mask((group < lower_bound) | (group > upper_bound))

# Group the data by the reference column and apply the remove_outliers function to each group
gdf_outliers_removed = gdf_concat_evi1[data_columns].groupby(gdf_concat_evi1[reference_column]).transform(remove_outliers)

# Create a copy of the original GeoDataFrame
gdf_filtered_evi = gdf_concat_evi1.copy()

# Replace the outlier values with NaN in the filtered GeoDataFrame
gdf_filtered_evi[data_columns] = gdf_outliers_removed
gdf_filtered_evi = gdf_filtered_evi.dropna()

# gdf_filtered_evi.head()

# Cleaning outlying values for JUST FOR NDVI
reference_column = 'crop_1'
data_columns = ['ndvi_1', 'ndvi_2', 'ndvi_3', 'ndvi_4', 'ndvi_5', 'ndvi_6']

# Remove minimally occurring indexes
labels_to_remove = ['onions', 'beans', 'rice', 'fallow', 'maize', 'vegetables']
#NOTE: '~' negates boolean, if there wasnt a '~' it would keep values correlating to 'lables_to_remove'
gdf_concat1 = gdf_concat_ndvi[~gdf_concat_ndvi['crop_1'].isin(labels_to_remove)]

# Calculate the lower and upper bounds for outlier removal
def remove_outliers(group):
    q1 = group.quantile(0.25)
    q3 = group.quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    return group.mask((group < lower_bound) | (group > upper_bound))

# Group the data by the reference column and apply the remove_outliers function to each group
gdf_outliers_removed = gdf_concat_ndvi[data_columns].groupby(gdf_concat_ndvi[reference_column]).transform(remove_outliers)

# Create a copy of the original GeoDataFrame
gdf_filtered_ndvi = gdf_concat_ndvi.copy()

# Replace the outlier values with NaN in the filtered GeoDataFrame
gdf_filtered_ndvi[data_columns] = gdf_outliers_removed
gdf_filtered_ndvi = gdf_filtered_ndvi.dropna()

# gdf_filtered_ndvi.head()

"""Concatenating cleaned gdfs"""

# Define the reference column and data columns
reference_column = 'crop_1'
data_columns = ['ndvi_1', 'ndvi_2', 'ndvi_3', 'ndvi_4', 'ndvi_5', 'ndvi_6', 'evi_1','evi_2','evi_3','evi_4','evi_5','evi_6',]

# Remove minimally occurring indexes
labels_to_remove = ['onions', 'beans', 'rice', 'fallow', 'maize', 'vegetables']
#NOTE: '~' negates boolean, if there wasnt a '~' it would keep values correlating to 'lables_to_remove'
gdf_concat = gdf_concat[~gdf_concat['crop_1'].isin(labels_to_remove)]

# Calculate the lower and upper bounds for outlier removal (e.g., using 1.5 times the interquartile range)
def remove_outliers(group):
    q1 = group.quantile(0.25)
    q3 = group.quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    return group.mask((group < lower_bound) | (group > upper_bound))

# Group the data by the reference column and apply the remove_outliers function to each group
gdf_outliers_removed = gdf_concat[data_columns].groupby(gdf_concat[reference_column]).transform(remove_outliers)

# Create a copy of the original GeoDataFrame
gdf_filtered = gdf_concat.copy()

# Replace the outlier values with NaN in the filtered GeoDataFrame
gdf_filtered[data_columns] = gdf_outliers_removed

gdf_filtered = gdf_filtered.dropna()
gdf_filtered_ndvi = gdf_filtered_ndvi.dropna()
gdf_filtered_evi = gdf_filtered_evi.dropna()

name_counts = gdf_filtered['crop_1'].value_counts()

# Create a bar plot of the name variations
name_counts.plot(kind='bar')

# Customize the plot
plt.xlabel('Name')
plt.ylabel('Count')
plt.title('Variations in Names')

# Rotate the x-axis labels for better readability
plt.xticks(rotation=45)

# Display the plot
plt.show()

"""#Subtask 9: Sub-task 9 - data visualisation: visualise input features (predictors).

A simple time series plot was chosen this task. There are 2 for each predictor. One comparing all predictor values in from each crop category over 6 months on one graph. Aswell as this, there is a series of linegraphs for each predictor for each crop category plotted individually. The time series plot suits this task the best because it displays:

1. any missing or exception/construed data per month

2. it's easily digestable for the reader and can present large amount of information concerning the data and the potential for that data to be used in a model with very little difficulty in understanding.

3. It can compare the different values within the predictors to one another and show whether there are any major differences in the values produced by each predictor category.

These visualisations supported the overall analysis workflow by showing that each band layer in each predictor produced, followed a uniform sequence and no unforeseen data collection issues happened during the data collection phase.

If I had more time, I would experiment with different months and identify which months produced the greatest variations between NDVI and EVI values according to the line graphs and tailor the model to fit those months.
"""

y_columns = ['ndvi_1', 'ndvi_2', 'ndvi_3', 'ndvi_4', 'ndvi_5', 'ndvi_6', 'evi_1','evi_2','evi_3','evi_4','evi_5','evi_6',]
evi_y = ['evi_1','evi_2','evi_3','evi_4','evi_5','evi_6']
ndvi_y = ['ndvi_1', 'ndvi_2', 'ndvi_3', 'ndvi_4', 'ndvi_5', 'ndvi_6']
reference_column = ['crop_1']

#Precleaned box plots EVI
# Extract the reference and data columns from the GeoDataFrame EVI ONLY
reference_values = gdf_filtered_evi[reference_column].squeeze()
data_values = gdf_filtered_evi[evi_y]

# Create separate box plots for each crop category
plt.figure(figsize=(10, 6))

for crop_category in reference_values.unique():
    crop_data = data_values[reference_values == crop_category]
    crop_data.boxplot(rot=45, patch_artist=True, boxprops=dict(facecolor='C0'))

    plt.xticks(range(1, len(evi_y) + 1), evi_y)
    plt.xlabel('EVI Columns')
    plt.ylabel('EVI Values')
    plt.title(f'Boxplot of NDVI Values for {crop_category}')
    plt.show()

#Precleaned box plots EVI
# Extract the reference and data columns from the GeoDataFrame NDVI ONLY
reference_values = gdf_filtered_ndvi[reference_column].squeeze()
data_values = gdf_filtered_ndvi[ndvi_y]

# Create separate box plots for each crop category
plt.figure(figsize=(10, 6))

for crop_category in reference_values.unique():
    crop_data = data_values[reference_values == crop_category]
    crop_data.boxplot(rot=45, patch_artist=True, boxprops=dict(facecolor='C0'))

    plt.xticks(range(1, len(ndvi_y) + 1), ndvi_y)
    plt.xlabel('EVI Columns')
    plt.ylabel('EVI Values')
    plt.title(f'Boxplot of NDVI Values for {crop_category}')
    plt.show()

#Precleaned box plots EVI
# Extract the reference and data columns from the GeoDataFrame All Together
reference_values = gdf_filtered[reference_column].squeeze()
data_values = gdf_filtered[y_columns]

# Create separate box plots for each crop category
plt.figure(figsize=(10, 6))

for crop_category in reference_values.unique():
    crop_data = data_values[reference_values == crop_category]
    crop_data.boxplot(rot=45, patch_artist=True, boxprops=dict(facecolor='C0'))

    plt.xticks(range(1, len(y_columns) + 1), y_columns)
    plt.xlabel('NDVI/EVI Columns')
    plt.ylabel('NDVI/EVI Values')
    plt.title(f'Boxplot of NDVI Values for {crop_category}')
    plt.show()

# Convert months list to a tuple
months = ('June', 'July', 'August', 'September', 'October', 'November')

# Step 2: Create a subset DataFrame with the reference column and the selected columns
subset_columns = reference_column + ndvi_y
subset_df = gdf_filtered[subset_columns]

# Step 3: Group the data by the reference column
grouped_df = subset_df.groupby(reference_column).mean()

# Step 4: Iterate over the categories in the reference column
for category in grouped_df.index:
    category_data = grouped_df.loc[category]

    # Step 5: Create the line plot for each category
    plt.plot(months, category_data, label=category)

# Step 6: Set the title and labels
plt.title('Mean NDVI Over Time by Crop Category')
plt.xlabel('Time')
plt.ylabel('Mean NDVI')

# Step 7: Show the plot
plt.legend()
plt.show()

# Convert months list to a tuple
months = ('June', 'July', 'August', 'September', 'October', 'November')

# Step 2: Create a subset DataFrame with the reference column and the selected columns
subset_columns = reference_column + evi_y
subset_df = gdf_filtered[subset_columns]

# Step 3: Group the data by the reference column
grouped_df = subset_df.groupby(reference_column).mean()

# Step 4: Iterate over the categories in the reference column
for category in grouped_df.index:
    category_data = grouped_df.loc[category]

    # Step 5: Create the line plot for each category
    plt.plot(months, category_data, label=category)

# Step 6: Set the title and labels
plt.title('Mean EVI Over Time by Crop Category')
plt.xlabel('Time')
plt.ylabel('Mean EVI')

# Step 7: Show the plot
plt.legend()
plt.show()

# Step 2: Create a subset DataFrame with the reference column and the selected columns
###NDVI ONLY
subset_df = gdf_filtered[reference_column + ndvi_y]

# Step 3: Group the data by the reference column
grouped_df = subset_df.groupby(reference_column).mean()

# Step 4: Iterate over the categories in the reference column
for category in grouped_df.index:
    category_data = grouped_df.loc[category]

    # Step 5: Create a new figure for each category
    plt.figure()
    sns.lineplot(x=months, y=category_data, label=category)

    # Step 6: Set the title and labels for each figure
    plt.title(f'Mean NDVI Over Time for {category}')
    plt.xlabel('Time')
    plt.ylabel('Mean NDVI')


    # Step 7: Show each individual figure
    plt.show()

# Step 2: Create a subset DataFrame with the reference column and the selected columns
###EVI ONLY
subset_df = gdf_filtered[reference_column + evi_y]

# Step 3: Group the data by the reference column
grouped_df = subset_df.groupby(reference_column).mean()

# Step 4: Iterate over the categories in the reference column
for category in grouped_df.index:
    category_data = grouped_df.loc[category]

    # Step 5: Create a new figure for each category
    plt.figure()
    sns.lineplot(x=months, y=category_data, label=category)

    # Step 6: Set the title and labels for each figure
    plt.title(f'Mean EVI Over Time for {category}')
    plt.xlabel('Time')
    plt.ylabel('Mean EVI')


    # Step 7: Show each individual figure
    plt.show()

"""#Subtask 10: Train Machine Learning Model"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.metrics import classification_report, accuracy_score
from sklearn.model_selection import cross_val_score
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.inspection import permutation_importance
rng = np.random.RandomState(0)

"""#Models For NDVI/EVI Only

* To give an idea of the efficacy of either as a standalone predictor
"""

##Model for NDVI only

# Split the data into features (X) and target (y)
X = gdf_filtered_ndvi.drop(['country', 'region', 'year', 'season', 'aoi', 'crop_1', 'crop1_code', 'geometry'], axis=1)
y = gdf_filtered_ndvi['crop1_code']  # Target

# Encode the target variable using label encoding
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a RandomForestClassifier model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
rf_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")
print(classification_report(y_test, y_pred))

# Calculate accuracy per class
accuracy_per_class = {}
for i, crop_class in enumerate(label_encoder.classes_):
    class_indices = y_test == i
    class_accuracy = accuracy_score(y_test[class_indices], y_pred[class_indices])
    accuracy_per_class[crop_class] = class_accuracy

print("Accuracy per Class:")
for crop_class, accuracy in accuracy_per_class.items():
    print(f"{crop_class}: {accuracy}")

# Perform cross-validation
cv_scores = cross_val_score(rf_model, X, y, cv=5, scoring='accuracy')

# Calculate the average accuracy
average_accuracy = cv_scores.mean()

print("Cross-Validation Accuracy:")
print(average_accuracy)

# Make predictions on the entire dataset
all_preds = rf_model.predict(X)

"""Model Using Only EVI"""

##Model for EVI only

# Split the data into features (X) and target (y)
X = gdf_filtered_evi.drop(['country', 'region', 'year', 'season', 'aoi', 'crop_1', 'crop1_code', 'geometry'], axis=1)
y = gdf_filtered_evi['crop1_code']  # Target

# Encode the target variable using label encoding
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a RandomForestClassifier model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
rf_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")
print(classification_report(y_test, y_pred))

# Calculate accuracy per class
accuracy_per_class = {}
for i, crop_class in enumerate(label_encoder.classes_):
    class_indices = y_test == i
    class_accuracy = accuracy_score(y_test[class_indices], y_pred[class_indices])
    accuracy_per_class[crop_class] = class_accuracy

print("Accuracy per Class:")
for crop_class, accuracy in accuracy_per_class.items():
    print(f"{crop_class}: {accuracy}")

# Perform cross-validation
cv_scores = cross_val_score(rf_model, X, y, cv=5, scoring='accuracy')

# Calculate the average accuracy
average_accuracy = cv_scores.mean()

print("Cross-Validation Accuracy:")
print(average_accuracy)

# Make predictions on the entire dataset
all_preds1 = rf_model.predict(X)

"""#Subtask 11: model: evaluate the machine learning model.

###1. A maximum of 250 words stating the metrics used to assess model performance and why they are suitable for this sub-task.

* accuracy_score(y_tst, y_pred): to measure percentage of correctly predicted crop classes out of all of the samples in the test set.

* classification_report(y_test, y_pred): to evaluate the performance of predicting each crop category using f-1 score, recall, support and precision.

* Accuracy_score: evaluates the accuracy of the model for each crop category.

* cross_val_score: uses accuracy to evaluate on 5 ('cv=5') different sets of generated sub 'folds' and then compares the accuracy between them. This is supposed to provide an estimation of how well this model would perform using a different dataset.

* ref_model.predict(x): makes predictions is used to store the models crop predictions so they can be applied to new data.


###2. A maximum of 250 words stating the strategy used to evaluate model performance and why it is suitable for this sub-task.

* This strategy was used to evaluate model performance because it gives a well rounded idea to the user as to why the model is behaving the way it is. It provides a breakdown of the accuracy of the model in relation to each crop. It provides a clear example of how many crops it correctly predicted over the whole dataset. It's suitabel for this task because it provides a quick and easily understandable evaluation of any classification model.

Executed code estimating the metrics for the model trained in sub-task 10.

Print the metrics scores.
"""

##Model for NDVI/EVI only

# Split the data into features (X) and target (y)
X = gdf_filtered.drop(['country', 'region', 'year', 'season', 'aoi', 'crop_1', 'crop1_code', 'geometry'], axis=1)
y = gdf_filtered['crop_1']  # Target

# Encode the target variable using label encoding
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a RandomForestClassifier model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
rf_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")
print(classification_report(y_test, y_pred))

# Calculate accuracy per class
accuracy_per_class = {}
for i, crop_1 in enumerate(label_encoder.classes_):
    class_indices = y_test == i
    class_accuracy = accuracy_score(y_test[class_indices], y_pred[class_indices])
    accuracy_per_class[crop_1] = class_accuracy

print("Accuracy per Class:")
for crop_1, accuracy in accuracy_per_class.items():
    print(f"{crop_1}: {accuracy}")

# Perform cross-validation
cv_scores = cross_val_score(rf_model, X, y, cv=5, scoring='accuracy')

# Calculate the average accuracy
average_accuracy = cv_scores.mean()

# print cross validation test scores
for i, mae in enumerate(cv_scores):
    print(f"the mae for the {i}th fold is {round(abs(mae), 4)}")

p_imp = permutation_importance(rf_model, X, y, scoring="accuracy", n_repeats=30, random_state=rng)
p_imp["importances_mean"]
columns = X.columns
p_imp = abs(p_imp["importances_mean"])
p_imp_df = pd.DataFrame({"feature": columns, "importance": p_imp})
p_imp_df = p_imp_df.sort_values(by=["importance"], ascending=True)
p_imp_df
fig = px.bar(p_imp_df, y="feature", x="importance", height=600)
print('Permutation Importance:')
fig.show()

# Make predictions on the entire dataset
all_preds2 = rf_model.predict(X)

labels = ["Wheat",
          "Alfalfa",
          "Vineyard",
          "Orchard",
          "Cotton"]
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
disp.plot(text_kw={"fontsize":10}, xticks_rotation="vertical")
plt.show()

"""#3. A maximum of 250 words commenting on your models suitability as a tool for generating crop type maps and suggestions for improving its predictive performance and adapting the model for deployment in new cropping landscapes.


* This model is moderately accurate for crop categories with sufficient data, so this model may be more suited to regions like germany where there would be more data for each category than central asia where data is sparse. To make the model more applicable to regions like central asia, the time period used to generate the predictors should be tailored to a period with maximum differentiation between each crop category. EVI has been shown to be a less accurate predictor for these categories than NDVI, this model could explore some other predictors including GNDVI, Leaf area index, Land surface temperature, and using individual spectral bands like shortwave infrared and near infrared to further develop the accuracy of the model. A combination of these predictors specifically leaf area index would help further differentiate, for example, horticultural crop categories from other crop types like grain and rice. Ground temperature would further increase differentiation between water hungry crop types like shallow water rice varieties. Seeing as the Orchard, Vineyard and Alfalfa categories had very similiar NDVI and EVI values across the time period, it would have been beenficial to group them together to increase data for a new super category. This model could also explore data from different sattelite providers, including MODIS.


"""

gdf_filtered_new = gdf_filtered.drop(['country', 'region', 'year', 'season', 'aoi'], axis=1)

class_mappings = {
    "0": "cotton",
    "1": "wheat",
    "2": "orchard",
    "3": "vineyard",
    "4": "alfalfa",
}


gdf_filtered_new["labels_cat"] = gdf_filtered_new["crop_1"].astype("str")

gdf_filtered_new.replace({"labels_cat": class_mappings}, inplace=True)

gdf_filtered_new.groupby("labels_cat").count().loc[:, "crop1_code"]

gdf_filtered_new["predicted"] = all_preds2.astype("str")

gdf_filtered_new.replace({"predicted": class_mappings}, inplace=True)
basemap = "https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}"
attribution = "Tiles &copy; Esri &mdash; Source: Esri, i-cubed, USDA, USGS, AEX, GeoEye, Getmapping, Aerogrid, IGN, IGP, UPR-EGP, and the GIS User Community"
gdf_filtered_new.explore(column="predicted", cmap="tab20", categorical=True, tiles=basemap, attr=attribution, tooltip=["labels_cat", "predicted"])

"""#References

* https://openai.com/blog/chatgpt
* https://www.usgs.gov/landsat-missionslandsat-enhanced-vegetation-index#:~:text=Landsat%20Enhanced%20Vegetation%20Index%20(EVI,in%20areas%20with%20dense%20vegetation.)
* https://www.sciencedirect.com/science/article/pii/S0034425706004949.
* https://www.sciencedirect.com/science/article/pii/S1470160X20310633.
"""